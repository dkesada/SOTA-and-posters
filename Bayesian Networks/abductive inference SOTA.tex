\documentclass[a4paper,11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[dvipdfmx]{graphicx}
\usepackage{graphics,latexsym}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage[dvips]{color}
\usepackage{subfigure}
\usepackage{verbatim}

\bibpunct{(}{)}{;}{a}{,}{,}

\textheight 24cm \textwidth 17cm \topmargin-2cm
%% \evensidemargin   -0.25cm
\oddsidemargin-0.2cm
%\pagestyle{empty}
\renewcommand{\baselinestretch}{1}

\begin{document}

\title{Abductive inference in Bayesian networks}

\author{{David Quesada López}\\
{\small Computational Intelligence Group, Departamento de Inteligencia Artificial, Universidad Polit\'ecnica de Madrid, Spain}}

\date{}
\maketitle

%\title{}

%\address{}

\begin{abstract} Abductive inference in Bayesian networks solves the problem of obtaining the most probable explanation (MPE) of a network given some evidence of its nodes. This inference can be total, if you aim to obtain the MPE of the whole network, or partial, if you are only interested in some of the nodes. In this state of the art we will cover both approaches and the methods used to solve them.
\end{abstract}


\ \\
KEY WORDS: Bayesian networks; Abductive inference; Approximate inference


\section{Introduction}

A Bayesian network is a directed acyclic graph (DAG) where you represent each node as a random variable and the edges represent the dependence or independence among them. Each node also has associated a conditional probability distribution conditioned to its parents that represents the probability distribution of the node's variable.

One of the problems associated with BNs is abductive inference, which consists in finding the maximum \textit{a posteriori} probability (MAP) state of the network given some evidence on the state of variables of the network. In general, we are interested in the \textit{K} most probable explanations (k-MPE), which is more complex (\cite{seroussi1994}). This problem is known to be NP-hard (\cite{shimony1994}), and most of the times trying to obtain an exact MPE turns out to be intractable. This is the reason why many researchers focus on getting \textit{K} approximate MPEs instead of the exact one.

Abductive inference can also be called total, when we want to know the whole network's k-MPEs, or partial, when we only want the \textit{K} most probable state assignments for a subset of the variables known as the explanation set 
(\cite{fortier2013}).

In this state of the art, we will review some methods to perform total and partial abductive inference on a Bayesian network.

\section{State-of-the-art}

We can formally describe the problem of the k-MPE as: given a set of unobserved variables $X_u$, a set of observed variables $X_o$, we want to find the configuration $x^*_u$ such that:

\begin{equation}
x^*_u = arg\max_{x_u} P(x_u | x_o)
\label{eqn:MPE}
\end{equation}

Which means we are aiming to maximize the MAP state of the network by finding an appropiate $x^*_u$ (\cite{deCampos2002}).

Traditionally, there has been three solutions to this problem (\cite{deCampos2001}): Through the computation of the \textit{chain rule}, which is intractable in even the simplest of networks, through belief propagation and through abductive inference.


\subsection{Total abductive inference}
It has been proven that belief propagation and abductive inference are both equal in complexity when finding the MPE of a network (\cite{dawid1992}), however the problem of finding the k-MPE is a more complex one.

There are two ways of tackling this problem:

\begin{itemize}
\item With exact computation. This is the case of \cite{dawid1992}, where a belief propagation algorithm was adapted to find the MPE of a network by replacing the summation by maximum in the message passing. This method was able to only find the first 3 MPEs of the network (\cite{nilsson1998}), so it had to be modified to be able to find the k-MPE. This involved using Dawid's algorithm and a divide and conquer algorithm that was used to find the remaining k-1 MPEs using the junction tree and the MPE from the first algorithm.

\item With approximate results. Here, we treat the problem as an optimization one. One approach was performed by \cite{gelsema1995} using a genetic algorithm (GA). In this method, each individual of the population represents a possible state of the network by representing the presence or absence of the nodes with a binary vector. Genetic programming is also used by \cite{rojas1993} to solve this problem. Here, they represent the network's graph in each individual. Another method was proposed by \cite{welch1996}, where he used a Montecarlo simulation to initialize the population and then used a GA similar to that of Gelsema. The most recent approach to this problem is with the use of overlapping swarm intelligence (\cite{fortier2013}), in which each swarm is assigned to a node of the network. Then, each swarm tries to find the state of its node for his markov blanket. Afterwards, the swarms that have the same node assigned compete with each other and to end the iteration the whole fitness of the network is calculated.
\end{itemize}

\subsection{Partial abductive inference}

In partial abductive inference, instead of having a set of unobserved variables $X_u$, we have an explanation set $X_E$ of the variables we are interested in and a set $X_R$ such as $X_R = X_u \textbackslash X_E$. With this, the the configuration $x^*_E$ we want to find is:

\begin{equation}
x^*_E = arg\max_{x_E} P(x_E | x_o) = arg\max_{x_E} \sum_{x_R}P(x_E,x_R|x_o)
\label{eqn:pMPE}
\end{equation}

This process is more complex than total abductive inference (\cite{deCampos2002}). They both are similar, but not all the clique trees obtained from the BN are valid, so the tree generation has to be modified from what we have in total abductive inference.

To solve this problem, there are the same approaches as in the total one:

\begin{itemize}

\item Trying to get the exact MPE. Here, the objective is trying to create an appropiate junction tree in which you can perform the methods of total abduction. The problem is that creating this modified junction trees is not trivial due to the non commutative behaviour of summation and maximum. There are methods that rely on variable elimination, like \cite{dechter1998} which uses his bucket elimination method to find the MPE and then we can use \cite{li1993} linear algorithm k-1 times to find the k-MPE. A variation of \cite{nilsson1998} regarding the formation of the trees can be used to solve partial abduction. 

\item Aproximating the best k-MPE. The main researcher in this area was performed De Campos \textit{et al.}, who started his research adapting a genetic algorithm to solve this problem as in the method followed by Gelsema (\cite{deCampos1999}). In this case, the chain rule cannot be used to evaluate the fitness of each individual, so it uses probability propagation. He also focused on improving the efficiency of this GA's operators by reusing part of the message propagation used in calculating the fitness of the parents when calculating the fitness of their offspring (\cite{deCampos2002}). Another alternative proposed was the use of simulated annealing (\cite{deCampos2001}) to move through the solution space. In this case, the cost function consists of the probability propagation that was used in the fitness function of the GA.

\end{itemize}

\section{Conclusions and future research}

Abductive inference in Bayesian networks is useful in many real case scenarios. Total abductive inference has been very studied by the literature and has a wide range of both exact and approximate methods to get the k-MPE of a network. Partial abductive inference is not as widely studied, but it seems like its main idea could be more useful than the total one. In both cases, the most recent advances are in the approximate branch, which is more feasible for bigger Bayesian networks.

The main open lines of research in this problem seem to be the development of more refined genetic algorithms for both total and partial, or the application of different methods for moving through the solution space of the maximization problem, as in the case of simulated annealing and overlapping swarm intelligence.

\bibliographystyle{plainnat}
\begin{thebibliography}{}

\bibitem[Dawid, 1992]{dawid1992}
Dawid A. P. (1992). Applications of a general propagation algorithm for probabilistic expert systems. Statistics and computing, 2(1), 25-36.

\bibitem[De Campos \textit{et al.}, 1999]{deCampos1999}
De Campos L. M., Gámez J. A., Moral S. (1999). Partial abductive inference in Bayesian belief networks using a genetic algorithm. Pattern Recognition Letters, 20(11), 1211-1217.

\bibitem[De Campos \textit{et al.}, 2001]{deCampos2001}
De Campos L. M., Gámez J. A., Moral S. (2001). Partial abductive inference in Bayesian belief networks by simulated annealing. International Journal of Approximate Reasoning, 27(3), 263-283.

\bibitem[De Campos \textit{et al.}, 2002]{deCampos2002}
De Campos L. M., Gamez J. A., Moral S. (2002). Partial abductive inference in Bayesian belief networks-an evolutionary computation approach by using problem-specific genetic operators. IEEE Transactions on Evolutionary Computation, 6(2), 105-131.

\bibitem[Dechter, 1998]{dechter1998}
Dechter R. (1998). Bucket elimination: A unifying framework for probabilistic inference. In Learning in graphical models (pp. 75-104). Springer, Dordrecht.

\bibitem[Fortier \textit{et al.}, 2013]{fortier2013}
Fortier N., Sheppard J., Pillai K. G. (2013, April). Bayesian abductive inference using overlapping swarm intelligence. In Swarm Intelligence (SIS), 2013 IEEE Symposium on (pp. 263-270). IEEE.

\bibitem[Gelsema, 1995]{gelsema1995}
Gelsema E. S. (1995). Abductive reasoning in Bayesian belief networks using a genetic algorithm. Pattern Recognition Letters, 16(8), 865-871.

\bibitem[Li and D'Ambrosio, 1993]{li1993}
Li Z., D'Ambrosio B. (1993, July). An efficient approach for finding the MPE in belief networks. In Proceedings of the Ninth international conference on Uncertainty in artificial intelligence (pp. 342-349). Morgan Kaufmann Publishers Inc..

\bibitem[Nilsson, 1998]{nilsson1998}
Nilsson D. (1998). An efficient algorithm for finding the M most probable configurations in probabilistic expert systems. Statistics and computing, 8(2), 159-173.

\bibitem[Rojas and Kramer, 1993]{rojas1993}
Rojas-Guzman C., Kramer M. A. (1993, July). Galgo: A genetic algorithm decision support tool for complex uncertain systems modeled with Bayesian belief networks. In Proceedings of the Ninth international conference on Uncertainty in artificial intelligence (pp. 368-375). Morgan Kaufmann Publishers Inc..

\bibitem[Seroussi and Golmard, 1994]{seroussi1994}
Seroussi B., Golmard J. L. (1994). An algorithm directly finding the K most probable configurations in Bayesian networks. International Journal of Approximate Reasoning, 11(3), 205-233.

\bibitem[Shimony, 1994]{shimony1994}
Shimony S. (1994). Finding maps for belief networks is NP-hard. Artificial Intelligence 68, 399-410.

\bibitem[Welch, 1996]{welch1996}
Welch R. L. (1996, August). Real time estimation of Bayesian networks. In Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence (pp. 533-544). Morgan Kaufmann Publishers Inc..

\end{thebibliography}


\end{document}
